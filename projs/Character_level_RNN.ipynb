{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pg5lLIrM_t5v"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum(axis=0)\n",
        "\n",
        "def smooth(loss, cur_loss):\n",
        "    return loss * 0.999 + cur_loss * 0.001\n",
        "\n",
        "def print_sample(sample_ix, ix_to_char):\n",
        "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
        "    txt = txt[0].upper() + txt[1:]  # capitalize first character\n",
        "    print ('%s' % (txt, ), end='')\n",
        "\n",
        "\n",
        "def get_sample(sample_ix, ix_to_char):\n",
        "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
        "    txt = txt[0].upper() + txt[1:]  # capitalize first character\n",
        "    return txt\n",
        "\n",
        "def get_initial_loss(vocab_size, seq_length):\n",
        "    return -np.log(1.0/vocab_size)*seq_length\n",
        "\n",
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum(axis=0)\n",
        "\n",
        "def initialize_parameters(n_a, n_x, n_y):\n",
        "    \"\"\"\n",
        "    Initialize parameters with small random values\n",
        "\n",
        "    Returns:\n",
        "    parameters -- python dictionary containing:\n",
        "                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
        "                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
        "                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
        "                        b --  Bias, numpy array of shape (n_a, 1)\n",
        "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
        "    \"\"\"\n",
        "    np.random.seed(1)\n",
        "    Wax = np.random.randn(n_a, n_x)*0.01 # input to hidden\n",
        "    Waa = np.random.randn(n_a, n_a)*0.01 # hidden to hidden\n",
        "    Wya = np.random.randn(n_y, n_a)*0.01 # hidden to output\n",
        "    b = np.zeros((n_a, 1)) # hidden bias\n",
        "    by = np.zeros((n_y, 1)) # output bias\n",
        "\n",
        "    parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"b\": b,\"by\": by}\n",
        "\n",
        "    return parameters\n",
        "\n",
        "def rnn_step_forward(parameters, a_prev, x):\n",
        "\n",
        "    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n",
        "    a_next = np.tanh(np.dot(Wax, x) + np.dot(Waa, a_prev) + b) # hidden state\n",
        "    p_t = softmax(np.dot(Wya, a_next) + by) # unnormalized log probabilities for next chars # probabilities for next chars\n",
        "\n",
        "    return a_next, p_t\n",
        "\n",
        "def rnn_step_backward(dy, gradients, parameters, x, a, a_prev):\n",
        "\n",
        "    gradients['dWya'] += np.dot(dy, a.T)\n",
        "    gradients['dby'] += dy\n",
        "    da = np.dot(parameters['Wya'].T, dy) + gradients['da_next'] # backprop into h\n",
        "    daraw = (1 - a * a) * da # backprop through tanh nonlinearity\n",
        "    gradients['db'] += daraw\n",
        "    gradients['dWax'] += np.dot(daraw, x.T)\n",
        "    gradients['dWaa'] += np.dot(daraw, a_prev.T)\n",
        "    gradients['da_next'] = np.dot(parameters['Waa'].T, daraw)\n",
        "    return gradients\n",
        "\n",
        "def update_parameters(parameters, gradients, lr):\n",
        "\n",
        "    parameters['Wax'] += -lr * gradients['dWax']\n",
        "    parameters['Waa'] += -lr * gradients['dWaa']\n",
        "    parameters['Wya'] += -lr * gradients['dWya']\n",
        "    parameters['b']  += -lr * gradients['db']\n",
        "    parameters['by']  += -lr * gradients['dby']\n",
        "    return parameters\n",
        "\n",
        "def rnn_forward(X, Y, a0, parameters, vocab_size = 27):\n",
        "\n",
        "    # Initialize x, a and y_hat as empty dictionaries\n",
        "    x, a, y_hat = {}, {}, {}\n",
        "\n",
        "    a[-1] = np.copy(a0)\n",
        "\n",
        "    # initialize your loss to 0\n",
        "    loss = 0\n",
        "\n",
        "    for t in range(len(X)):\n",
        "\n",
        "        # Set x[t] to be the one-hot vector representation of the t'th character in X.\n",
        "        # if X[t] == None, we just have x[t]=0. This is used to set the input for the first timestep to the zero vector.\n",
        "        x[t] = np.zeros((vocab_size,1))\n",
        "        if (X[t] != None):\n",
        "            x[t][X[t]] = 1\n",
        "\n",
        "        # Run one step forward of the RNN\n",
        "        a[t], y_hat[t] = rnn_step_forward(parameters, a[t-1], x[t])\n",
        "\n",
        "        # Update the loss by substracting the cross-entropy term of this time-step from it.\n",
        "        loss -= np.log(y_hat[t][Y[t],0])\n",
        "\n",
        "    cache = (y_hat, a, x)\n",
        "\n",
        "    return loss, cache\n",
        "\n",
        "def rnn_backward(X, Y, parameters, cache):\n",
        "    # Initialize gradients as an empty dictionary\n",
        "    gradients = {}\n",
        "\n",
        "    # Retrieve from cache and parameters\n",
        "    (y_hat, a, x) = cache\n",
        "    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n",
        "\n",
        "    # each one should be initialized to zeros of the same dimension as its corresponding parameter\n",
        "    gradients['dWax'], gradients['dWaa'], gradients['dWya'] = np.zeros_like(Wax), np.zeros_like(Waa), np.zeros_like(Wya)\n",
        "    gradients['db'], gradients['dby'] = np.zeros_like(b), np.zeros_like(by)\n",
        "    gradients['da_next'] = np.zeros_like(a[0])\n",
        "\n",
        "    # Backpropagate through time\n",
        "    for t in reversed(range(len(X))):\n",
        "        dy = np.copy(y_hat[t])\n",
        "        dy[Y[t]] -= 1\n",
        "        gradients = rnn_step_backward(dy, gradients, parameters, x[t], a[t], a[t-1])\n",
        "\n",
        "\n",
        "    return gradients, a"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random"
      ],
      "metadata": {
        "id": "gZrvSS4w_zf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = open('names.txt', 'r').read()\n",
        "data= data.lower()\n",
        "chars = list(set(data))\n",
        "data_size, vocab_size = len(data), len(chars)\n",
        "print('There are %d total characters and %d unique characters in your data.' % (data_size, vocab_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGP0tf9uCiAZ",
        "outputId": "1173d9d4-6bf5-486b-8542-069f90987327"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 893350 total characters and 27 unique characters in your data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(chars)\n",
        "print(chars)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPkP-14JDR4X",
        "outputId": "3138201b-81ee-4192-b6c2-84c1a8048e76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pprint\n",
        "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
        "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
        "pp = pprint.PrettyPrinter(indent=4)\n",
        "pp.pprint(ix_to_char)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxaWTy3rDk22",
        "outputId": "020148ec-8ca6-4db7-e1f5-0fd85c10b7b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{   0: '\\n',\n",
            "    1: 'a',\n",
            "    2: 'b',\n",
            "    3: 'c',\n",
            "    4: 'd',\n",
            "    5: 'e',\n",
            "    6: 'f',\n",
            "    7: 'g',\n",
            "    8: 'h',\n",
            "    9: 'i',\n",
            "    10: 'j',\n",
            "    11: 'k',\n",
            "    12: 'l',\n",
            "    13: 'm',\n",
            "    14: 'n',\n",
            "    15: 'o',\n",
            "    16: 'p',\n",
            "    17: 'q',\n",
            "    18: 'r',\n",
            "    19: 's',\n",
            "    20: 't',\n",
            "    21: 'u',\n",
            "    22: 'v',\n",
            "    23: 'w',\n",
            "    24: 'x',\n",
            "    25: 'y',\n",
            "    26: 'z'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "### GRADED FUNCTION: clip\n",
        "import copy\n",
        "def clip(gradients, maxValue):\n",
        "    '''\n",
        "    Clips the gradients' values between minimum and maximum.\n",
        "\n",
        "    Arguments:\n",
        "    gradients -- a dictionary containing the gradients \"dWaa\", \"dWax\", \"dWya\", \"db\", \"dby\"\n",
        "    maxValue -- everything above this number is set to this number, and everything less than -maxValue is set to -maxValue\n",
        "\n",
        "    Returns:\n",
        "    gradients -- a dictionary with the clipped gradients.\n",
        "    '''\n",
        "    gradients = copy.deepcopy(gradients)\n",
        "\n",
        "    dWaa, dWax, dWya, db, dby = gradients['dWaa'], gradients['dWax'], gradients['dWya'], gradients['db'], gradients['dby']\n",
        "\n",
        "    # Clip to mitigate exploding gradients, loop over [dWax, dWaa, dWya, db, dby]. (≈2 lines)\n",
        "    for gradient in gradients:\n",
        "        np.clip(gradients[gradient], -maxValue, maxValue, out = gradients[gradient])\n",
        "\n",
        "    gradients = {\"dWaa\": dWaa, \"dWax\": dWax, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n",
        "\n",
        "    return gradients"
      ],
      "metadata": {
        "id": "ymBJXqbHE5aR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(parameters, char_to_ix, seed):\n",
        "    \"\"\"\n",
        "    Sample a sequence of characters according to a sequence of probability distributions output of the RNN\n",
        "\n",
        "    Arguments:\n",
        "    parameters -- Python dictionary containing the parameters Waa, Wax, Wya, by, and b.\n",
        "    char_to_ix -- Python dictionary mapping each character to an index.\n",
        "    seed -- Used for grading purposes. Do not worry about it.\n",
        "\n",
        "    Returns:\n",
        "    indices -- A list of length n containing the indices of the sampled characters.\n",
        "    \"\"\"\n",
        "\n",
        "    # Retrieve parameters and relevant shapes from \"parameters\" dictionary\n",
        "    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n",
        "    vocab_size = by.shape[0]\n",
        "    n_a = Waa.shape[1]\n",
        "\n",
        "    # Step 1: Create the a zero vector x that can be used as the one-hot vector\n",
        "    # Representing the first character (initializing the sequence generation). (≈1 line)\n",
        "    x = np.zeros((vocab_size,1))\n",
        "    # Step 1': Initialize a_prev as zeros (≈1 line)\n",
        "    a_prev = np.zeros((n_a ,1))\n",
        "\n",
        "    # Create an empty list of indices. This is the list which will contain the list of indices of the characters to generate (≈1 line)\n",
        "    indices = []\n",
        "\n",
        "    # idx is the index of the one-hot vector x that is set to 1\n",
        "    # All other positions in x are zero.\n",
        "    # Initialize idx to -1\n",
        "    idx = -1\n",
        "\n",
        "    # Loop over time-steps t. At each time-step:\n",
        "    # Sample a character from a probability distribution\n",
        "    # And append its index (`idx`) to the list \"indices\".\n",
        "    # You'll stop if you reach 50 characters\n",
        "    # (which should be very unlikely with a well-trained model).\n",
        "    # Setting the maximum number of characters helps with debugging and prevents infinite loops.\n",
        "    counter = 0\n",
        "    newline_character = char_to_ix['\\n']\n",
        "\n",
        "    while (idx != newline_character and counter != 50):\n",
        "\n",
        "        # Step 2: Forward propagate x using the equations (1), (2) and (3)\n",
        "        a = np.tanh(np.dot(Wax,x) + np.dot(Waa,a_prev) + b)\n",
        "        z = np.dot(Wya,a) + by\n",
        "        y = softmax(z)\n",
        "\n",
        "        # For grading purposes\n",
        "        np.random.seed(counter + seed)\n",
        "\n",
        "        # Step 3: Sample the index of a character within the vocabulary from the probability distribution y\n",
        "        # (see additional hints above)\n",
        "        idx = np.random.choice(range(len(y)), p = np.squeeze(y) )\n",
        "\n",
        "        # Append the index to \"indices\"\n",
        "        indices.append(idx)\n",
        "\n",
        "        # Step 4: Overwrite the input x with one that corresponds to the sampled index `idx`.\n",
        "        # (see additional hints above)\n",
        "        x = np.zeros((vocab_size,1))\n",
        "        x[idx] = 1\n",
        "\n",
        "        # Update \"a_prev\" to be \"a\"\n",
        "        a_prev = a\n",
        "\n",
        "        # for grading purposes\n",
        "        seed += 1\n",
        "\n",
        "        counter +=1\n",
        "\n",
        "\n",
        "    if (counter == 50):\n",
        "        indices.append(char_to_ix['\\n'])\n",
        "\n",
        "    return indices"
      ],
      "metadata": {
        "id": "x_Rk3wqTFQBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "# GRADED FUNCTION: optimize\n",
        "\n",
        "def optimize(X, Y, a_prev, parameters, learning_rate = 0.01):\n",
        "    \"\"\"\n",
        "\n",
        "    Arguments:\n",
        "    X -- list of integers, where each integer is a number that maps to a character in the vocabulary.\n",
        "    Y -- list of integers, exactly the same as X but shifted one index to the left.\n",
        "    a_prev -- previous hidden state.\n",
        "    parameters -- python dictionary containing:\n",
        "                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
        "                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
        "                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
        "                        b --  Bias, numpy array of shape (n_a, 1)\n",
        "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
        "    learning_rate -- learning rate for the model.\n",
        "\n",
        "    Returns:\n",
        "    loss -- value of the loss function (cross-entropy)\n",
        "    gradients -- python dictionary containing:\n",
        "                        dWax -- Gradients of input-to-hidden weights, of shape (n_a, n_x)\n",
        "                        dWaa -- Gradients of hidden-to-hidden weights, of shape (n_a, n_a)\n",
        "                        dWya -- Gradients of hidden-to-output weights, of shape (n_y, n_a)\n",
        "                        db -- Gradients of bias vector, of shape (n_a, 1)\n",
        "                        dby -- Gradients of output bias vector, of shape (n_y, 1)\n",
        "    a[len(X)-1] -- the last hidden state, of shape (n_a, 1)\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    # Forward propagate through time (≈1 line)\n",
        "    loss, cache = rnn_forward(X, Y, a_prev, parameters)\n",
        "\n",
        "    # Backpropagate through time (≈1 line)\n",
        "    gradients, a = rnn_backward(X, Y, parameters, cache)\n",
        "\n",
        "    # Clip your gradients between -5 (min) and 5 (max) (≈1 line)\n",
        "    gradients = clip(gradients, 5)\n",
        "\n",
        "    # Update parameters (≈1 line)\n",
        "    parameters = update_parameters(parameters, gradients, learning_rate)\n",
        "\n",
        "\n",
        "    return loss, gradients, a[len(X)-1]"
      ],
      "metadata": {
        "id": "_eR9eQXNFXLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "# GRADED FUNCTION: model\n",
        "\n",
        "def model(data_x, ix_to_char, char_to_ix, num_iterations = 35000, n_a = 50, dino_names = 7, vocab_size = 27, verbose = False):\n",
        "    \"\"\"\n",
        "\n",
        "    Arguments:\n",
        "    data_x -- text corpus, divided in words\n",
        "    ix_to_char -- dictionary that maps the index to a character\n",
        "    char_to_ix -- dictionary that maps a character to an index\n",
        "    num_iterations -- number of iterations to train the model for\n",
        "    n_a -- number of units of the RNN cell\n",
        "    dino_names -- number of dinosaur names you want to sample at each iteration.\n",
        "    vocab_size -- number of unique characters found in the text (size of the vocabulary)\n",
        "\n",
        "    Returns:\n",
        "    parameters -- learned parameters\n",
        "    \"\"\"\n",
        "\n",
        "    # Retrieve n_x and n_y from vocab_size\n",
        "    n_x, n_y = vocab_size, vocab_size\n",
        "\n",
        "    # Initialize parameters\n",
        "    parameters = initialize_parameters(n_a, n_x, n_y)\n",
        "\n",
        "    # Initialize loss (this is required because we want to smooth our loss)\n",
        "    loss = get_initial_loss(vocab_size, dino_names)\n",
        "\n",
        "    # Build list of all dinosaur names (training examples).\n",
        "    examples = [x.strip() for x in data_x]\n",
        "\n",
        "    # Shuffle list of all dinosaur names\n",
        "    np.random.seed(0)\n",
        "    np.random.shuffle(examples)\n",
        "\n",
        "    # Initialize the hidden state of your LSTM\n",
        "    a_prev = np.zeros((n_a, 1))\n",
        "\n",
        "    # for grading purposes\n",
        "    last_dino_name = \"abc\"\n",
        "\n",
        "    # Optimization loop\n",
        "    for j in range(num_iterations):\n",
        "\n",
        "\n",
        "        # Set the index `idx` (see instructions above)\n",
        "        idx = j%len(examples)\n",
        "\n",
        "        # Set the input X (see instructions above)\n",
        "        single_example_chars = examples[idx]\n",
        "        single_example_ix = [char_to_ix[c] for c in single_example_chars]\n",
        "\n",
        "        # if X[t] == None, we just have x[t]=0. This is used to set the input for the first timestep to the zero vector.\n",
        "        X = [None] + single_example_ix\n",
        "\n",
        "        # Set the labels Y (see instructions above)\n",
        "        # The goal is to train the RNN to predict the next letter in the name\n",
        "        # So the labels are the list of characters that are one time-step ahead of the characters in the input X\n",
        "        Y = X[1:]\n",
        "        # The RNN should predict a newline at the last letter, so add ix_newline to the end of the labels\n",
        "        ix_newline = [char_to_ix[\"\\n\"]]\n",
        "        Y = Y + ix_newline\n",
        "\n",
        "        # Perform one optimization step: Forward-prop -> Backward-prop -> Clip -> Update parameters\n",
        "        # Choose a learning rate of 0.01\n",
        "        curr_loss, gradients, a_prev = optimize(X, Y, a_prev, parameters, learning_rate = 0.01)\n",
        "\n",
        "\n",
        "        # debug statements to aid in correctly forming X, Y\n",
        "        if verbose and j in [0, len(examples) -1, len(examples)]:\n",
        "            print(\"j = \" , j, \"idx = \", idx,)\n",
        "        if verbose and j in [0]:\n",
        "            #print(\"single_example =\", single_example)\n",
        "            print(\"single_example_chars\", single_example_chars)\n",
        "            print(\"single_example_ix\", single_example_ix)\n",
        "            print(\" X = \", X, \"\\n\", \"Y =       \", Y, \"\\n\")\n",
        "\n",
        "        # Use a latency trick to keep the loss smooth. It happens here to accelerate the training.\n",
        "        loss = smooth(loss, curr_loss)\n",
        "\n",
        "        # Every 1000 Iteration, generate \"n\" characters thanks to sample() to check if the model is learning properly\n",
        "        if j % 1000 == 0:\n",
        "\n",
        "            print('Iteration: %d, Loss: %f' % (j, loss) + '\\n')\n",
        "\n",
        "            # The number of dinosaur names to print\n",
        "            seed = 0\n",
        "            for name in range(dino_names):\n",
        "\n",
        "                # Sample indices and print them\n",
        "                sampled_indices = sample(parameters, char_to_ix, seed)\n",
        "                last_dino_name = get_sample(sampled_indices, ix_to_char)\n",
        "                print(last_dino_name.replace('\\n', ''))\n",
        "\n",
        "                seed += 1  # To get the same result (for grading purposes), increment the seed by one.\n",
        "\n",
        "            print('\\n')\n",
        "\n",
        "    return parameters, last_dino_name"
      ],
      "metadata": {
        "id": "TdBLqxnkFbLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parameters, last_name = model(data.split(\"\\n\"), ix_to_char, char_to_ix, 100000, verbose = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sspioPoHFes5",
        "outputId": "9833c6ba-446d-4c8f-c282-fac206818ba4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "j =  0 idx =  0\n",
            "single_example_chars olfrun\n",
            "single_example_ix [15, 12, 6, 18, 21, 14]\n",
            " X =  [None, 15, 12, 6, 18, 21, 14] \n",
            " Y =        [15, 12, 6, 18, 21, 14, 0] \n",
            "\n",
            "Iteration: 0, Loss: 23.070859\n",
            "\n",
            "Nkzxwtdmfqoeyhsqwasjkjvu\n",
            "Kneb\n",
            "Kzxwtdmfqoeyhsqwasjkjvu\n",
            "Neb\n",
            "Zxwtdmfqoeyhsqwasjkjvu\n",
            "Eb\n",
            "Xwtdmfqoeyhsqwasjkjvu\n",
            "\n",
            "\n",
            "Iteration: 1000, Loss: 20.872894\n",
            "\n",
            "Liwutrajanmaueont\n",
            "Ila\n",
            "Ixutrajanmaueont\n",
            "La\n",
            "Xutrajanmaueont\n",
            "A\n",
            "Utrajanmaueont\n",
            "\n",
            "\n",
            "Iteration: 2000, Loss: 19.566700\n",
            "\n",
            "Jixtso\n",
            "Hia\n",
            "Ixtso\n",
            "Ja\n",
            "Xtso\n",
            "A\n",
            "Uto\n",
            "\n",
            "\n",
            "Iteration: 3000, Loss: 18.596351\n",
            "\n",
            "Iiyuto\n",
            "Hia\n",
            "Hutto\n",
            "I\n",
            "Vtto\n",
            "A\n",
            "Uto\n",
            "\n",
            "\n",
            "Iteration: 4000, Loss: 17.977372\n",
            "\n",
            "Iiwuto\n",
            "En\n",
            "Ewuto\n",
            "I\n",
            "Utto\n",
            "A\n",
            "Uto\n",
            "\n",
            "\n",
            "Iteration: 5000, Loss: 17.497498\n",
            "\n",
            "Iixturalani\n",
            "Hica\n",
            "Huuto\n",
            "Ia\n",
            "Vuto\n",
            "Ba\n",
            "Uto\n",
            "\n",
            "\n",
            "Iteration: 6000, Loss: 17.291524\n",
            "\n",
            "Ikusurakami\n",
            "Fica\n",
            "Hutstamani\n",
            "Ica\n",
            "Utsuamani\n",
            "A\n",
            "Uto\n",
            "\n",
            "\n",
            "Iteration: 7000, Loss: 17.310151\n",
            "\n",
            "Ilwutoalanheubino\n",
            "En\n",
            "Guutoalanheubino\n",
            "Ie\n",
            "Uutoalanheubino\n",
            "Aa\n",
            "Utoalanheubino\n",
            "\n",
            "\n",
            "Iteration: 8000, Loss: 16.998665\n",
            "\n",
            "Ilyutocherkaujiou\n",
            "Eoe\n",
            "Huutoan\n",
            "Ifa\n",
            "Uuusamaorawarou\n",
            "Abaerse\n",
            "Uusamaorawarou\n",
            "\n",
            "\n",
            "Iteration: 9000, Loss: 16.695214\n",
            "\n",
            "Ilyusu\n",
            "Hie\n",
            "Huuto\n",
            "Iga\n",
            "Vuur\n",
            "Ab\n",
            "Uur\n",
            "\n",
            "\n",
            "Iteration: 10000, Loss: 16.876937\n",
            "\n",
            "Ikutorakenibuiris\n",
            "Hifa\n",
            "Huutodicimeuhino\n",
            "Iha\n",
            "Wutoelaqkeuipis\n",
            "Acahire\n",
            "Utoelbinaxerko\n",
            "\n",
            "\n",
            "Iteration: 11000, Loss: 16.698451\n",
            "\n",
            "Ikusuragaoraterir\n",
            "Hida\n",
            "Hutssalangaterjo\n",
            "Iga\n",
            "Uuto\n",
            "Acaero\n",
            "Uto\n",
            "\n",
            "\n",
            "Iteration: 12000, Loss: 16.421293\n",
            "\n",
            "Ikutoma\n",
            "Hid\n",
            "Iyuuo\n",
            "Iga\n",
            "Uuto\n",
            "Acador\n",
            "Uusai\n",
            "\n",
            "\n",
            "Iteration: 13000, Loss: 16.434737\n",
            "\n",
            "Imytsucherkauirlr\n",
            "Hiha\n",
            "Hyuss\n",
            "Iha\n",
            "Xuts\n",
            "Ad\n",
            "Uusanangawerot\n",
            "\n",
            "\n",
            "Iteration: 14000, Loss: 16.344561\n",
            "\n",
            "Ikustsanalg\n",
            "Enca\n",
            "Huuso\n",
            "Iga\n",
            "Vuuo\n",
            "Acabosa\n",
            "Uuo\n",
            "\n",
            "\n",
            "Iteration: 15000, Loss: 16.700480\n",
            "\n",
            "Ikutoramaro\n",
            "Higa\n",
            "Huuto\n",
            "Iga\n",
            "Vuuragarocterou\n",
            "Acaerrd\n",
            "Uusamerlawdors\n",
            "\n",
            "\n",
            "Iteration: 16000, Loss: 16.563849\n",
            "\n",
            "Ilwrusamaou\n",
            "Hie\n",
            "Huuusamaou\n",
            "Iga\n",
            "Vuto\n",
            "Acacusi\n",
            "Uusamaotaukoou\n",
            "\n",
            "\n",
            "Iteration: 17000, Loss: 16.276833\n",
            "\n",
            "Ikusto\n",
            "Hie\n",
            "Hutpramang\n",
            "Iea\n",
            "Vrura\n",
            "Abaersa\n",
            "Uura\n",
            "\n",
            "\n",
            "Iteration: 18000, Loss: 15.968277\n",
            "\n",
            "Ikusura\n",
            "Ina\n",
            "Izussan\n",
            "Ie\n",
            "Uuto\n",
            "Aca\n",
            "Uusai\n",
            "\n",
            "\n",
            "Iteration: 19000, Loss: 16.181692\n",
            "\n",
            "Ikutor\n",
            "Ina\n",
            "Izuto\n",
            "Ida\n",
            "Vuusakder\n",
            "Ebaerra\n",
            "Uutamani\n",
            "\n",
            "\n",
            "Iteration: 20000, Loss: 16.212012\n",
            "\n",
            "Ilwoto\n",
            "Hida\n",
            "Izuto\n",
            "Ie\n",
            "Wsusakaen\n",
            "Acabota\n",
            "Uusakaen\n",
            "\n",
            "\n",
            "Iteration: 21000, Loss: 16.716995\n",
            "\n",
            "Ilwsusaka\n",
            "Inba\n",
            "Izuto\n",
            "Iga\n",
            "Wuto\n",
            "Ac\n",
            "Uus\n",
            "\n",
            "\n",
            "Iteration: 22000, Loss: 16.213782\n",
            "\n",
            "Ilwroraganfawario\n",
            "Hie\n",
            "Hutor\n",
            "Ie\n",
            "Yutn\n",
            "Aca\n",
            "Uupaka\n",
            "\n",
            "\n",
            "Iteration: 23000, Loss: 16.054248\n",
            "\n",
            "Invsura\n",
            "Ina\n",
            "Izusr\n",
            "Iga\n",
            "Yuur\n",
            "Ac\n",
            "Uura\n",
            "\n",
            "\n",
            "Iteration: 24000, Loss: 15.964663\n",
            "\n",
            "Imusura\n",
            "Hie\n",
            "Izuss\n",
            "Iga\n",
            "Vuura\n",
            "Ad\n",
            "Uusan\n",
            "\n",
            "\n",
            "Iteration: 25000, Loss: 16.130569\n",
            "\n",
            "Ikutorahern\n",
            "Inca\n",
            "Izuto\n",
            "Iga\n",
            "Yuura\n",
            "Ad\n",
            "Uusake\n",
            "\n",
            "\n",
            "Iteration: 26000, Loss: 15.769130\n",
            "\n",
            "Ikusus\n",
            "Ima\n",
            "Iyust\n",
            "Iga\n",
            "Yuto\n",
            "Acaart\n",
            "Uuradang\n",
            "\n",
            "\n",
            "Iteration: 27000, Loss: 16.043182\n",
            "\n",
            "Ilytsucheraguko\n",
            "Ina\n",
            "Iyusu\n",
            "Ie\n",
            "Yuuragaon\n",
            "Acabou\n",
            "Uurae\n",
            "\n",
            "\n",
            "Iteration: 28000, Loss: 16.284372\n",
            "\n",
            "Ikutosemene\n",
            "Ina\n",
            "Izuto\n",
            "Iea\n",
            "Yuusakeilau\n",
            "Ea\n",
            "Uusakeilau\n",
            "\n",
            "\n",
            "Iteration: 29000, Loss: 16.323371\n",
            "\n",
            "Imutsu\n",
            "Ina\n",
            "Izust\n",
            "Iha\n",
            "Yust\n",
            "Eb\n",
            "Uura\n",
            "\n",
            "\n",
            "Iteration: 30000, Loss: 16.113502\n",
            "\n",
            "Ikusto\n",
            "Ina\n",
            "Izuspamapi\n",
            "Ie\n",
            "Yuto\n",
            "Ad\n",
            "Uura\n",
            "\n",
            "\n",
            "Iteration: 31000, Loss: 16.383425\n",
            "\n",
            "Ilxsura\n",
            "Ina\n",
            "Izuro\n",
            "Ie\n",
            "Yustan\n",
            "Ad\n",
            "Uurae\n",
            "\n",
            "\n",
            "Iteration: 32000, Loss: 15.982184\n",
            "\n",
            "Liyust\n",
            "Ina\n",
            "Izuto\n",
            "Lda\n",
            "Yuura\n",
            "Eb\n",
            "Uusa\n",
            "\n",
            "\n",
            "Iteration: 33000, Loss: 15.839819\n",
            "\n",
            "Liysus\n",
            "Ina\n",
            "Izuso\n",
            "Lda\n",
            "Yuura\n",
            "Ecadosa\n",
            "Uus\n",
            "\n",
            "\n",
            "Iteration: 34000, Loss: 16.007869\n",
            "\n",
            "Ikutoka\n",
            "Imba\n",
            "Izuto\n",
            "Iha\n",
            "Yuura\n",
            "Ec\n",
            "Uura\n",
            "\n",
            "\n",
            "Iteration: 35000, Loss: 16.142155\n",
            "\n",
            "Miwoun\n",
            "Ima\n",
            "Iyuura\n",
            "Ma\n",
            "Yuura\n",
            "Ea\n",
            "Uusamder\n",
            "\n",
            "\n",
            "Iteration: 36000, Loss: 15.696913\n",
            "\n",
            "Ikutppamami\n",
            "Hie\n",
            "Huuso\n",
            "Iga\n",
            "Yuura\n",
            "Acaano\n",
            "Uura\n",
            "\n",
            "\n",
            "Iteration: 37000, Loss: 15.756452\n",
            "\n",
            "Ilvtsuakane\n",
            "Hid\n",
            "Izusu\n",
            "Ie\n",
            "Yuura\n",
            "Aca\n",
            "Uusakeil\n",
            "\n",
            "\n",
            "Iteration: 38000, Loss: 15.919422\n",
            "\n",
            "Ikuton\n",
            "Ima\n",
            "Izuto\n",
            "Iia\n",
            "Yuusalage\n",
            "Ed\n",
            "Uusake\n",
            "\n",
            "\n",
            "Iteration: 39000, Loss: 15.949476\n",
            "\n",
            "Ikusun\n",
            "Ima\n",
            "Izusu\n",
            "Iga\n",
            "Yussanane\n",
            "Acadota\n",
            "Uto\n",
            "\n",
            "\n",
            "Iteration: 40000, Loss: 16.017131\n",
            "\n",
            "Liytsube\n",
            "Inda\n",
            "Izusuei\n",
            "Lda\n",
            "Yuto\n",
            "Ad\n",
            "Uto\n",
            "\n",
            "\n",
            "Iteration: 41000, Loss: 15.792261\n",
            "\n",
            "Kiyuni\n",
            "Ina\n",
            "Izust\n",
            "Ki\n",
            "Yust\n",
            "Aca\n",
            "Utson\n",
            "\n",
            "\n",
            "Iteration: 42000, Loss: 15.746385\n",
            "\n",
            "Niyuor\n",
            "Ina\n",
            "Izusu\n",
            "Oka\n",
            "Yusu\n",
            "Eb\n",
            "Uura\n",
            "\n",
            "\n",
            "Iteration: 43000, Loss: 16.104459\n",
            "\n",
            "Ikuton\n",
            "Inda\n",
            "Izuto\n",
            "Iga\n",
            "Vuto\n",
            "Aca\n",
            "Uura\n",
            "\n",
            "\n",
            "Iteration: 44000, Loss: 15.747739\n",
            "\n",
            "Ilyusr\n",
            "Inca\n",
            "Izuss\n",
            "Ie\n",
            "Uust\n",
            "Ad\n",
            "Utshi\n",
            "\n",
            "\n",
            "Iteration: 45000, Loss: 15.910760\n",
            "\n",
            "Ilyuro\n",
            "Ind\n",
            "Iyuto\n",
            "Iga\n",
            "Wustan\n",
            "Acairs\n",
            "Utrde\n",
            "\n",
            "\n",
            "Iteration: 46000, Loss: 15.610560\n",
            "\n",
            "Inyuupal\n",
            "Ina\n",
            "Iyuso\n",
            "Ii\n",
            "Yuto\n",
            "Ag\n",
            "Uura\n",
            "\n",
            "\n",
            "Iteration: 47000, Loss: 15.941004\n",
            "\n",
            "Ilysusai\n",
            "In\n",
            "Iyuto\n",
            "Iga\n",
            "Wuss\n",
            "Ec\n",
            "Uusae\n",
            "\n",
            "\n",
            "Iteration: 48000, Loss: 15.964768\n",
            "\n",
            "Imusurcher\n",
            "Ina\n",
            "Izustara\n",
            "Iga\n",
            "Vustao\n",
            "Ce\n",
            "Uure\n",
            "\n",
            "\n",
            "Iteration: 49000, Loss: 15.935746\n",
            "\n",
            "Inyuura\n",
            "Ina\n",
            "Ixuto\n",
            "Iga\n",
            "Yuura\n",
            "Ad\n",
            "Uura\n",
            "\n",
            "\n",
            "Iteration: 50000, Loss: 15.956214\n",
            "\n",
            "Kiyuru\n",
            "Ina\n",
            "Izusu\n",
            "Ka\n",
            "Yuss\n",
            "Ad\n",
            "Uura\n",
            "\n",
            "\n",
            "Iteration: 51000, Loss: 15.847696\n",
            "\n",
            "Imuto\n",
            "Hi\n",
            "Iysura\n",
            "Iga\n",
            "Yuto\n",
            "Aca\n",
            "Uusaka\n",
            "\n",
            "\n",
            "Iteration: 52000, Loss: 15.653392\n",
            "\n",
            "Imsuro\n",
            "Ena\n",
            "Extos\n",
            "Iha\n",
            "Uuso\n",
            "Ad\n",
            "Uto\n",
            "\n",
            "\n",
            "Iteration: 53000, Loss: 15.703023\n",
            "\n",
            "Ilvuto\n",
            "Ina\n",
            "Iyuto\n",
            "Iha\n",
            "Yuto\n",
            "Ae\n",
            "Uura\n",
            "\n",
            "\n",
            "Iteration: 54000, Loss: 16.039593\n",
            "\n",
            "Imutora\n",
            "Ina\n",
            "Izuso\n",
            "Iga\n",
            "Yuun\n",
            "Ad\n",
            "Uura\n",
            "\n",
            "\n",
            "Iteration: 55000, Loss: 15.754078\n",
            "\n",
            "Inusus\n",
            "Ina\n",
            "Iyusode\n",
            "Ie\n",
            "Yuun\n",
            "Ad\n",
            "Uuo\n",
            "\n",
            "\n",
            "Iteration: 56000, Loss: 15.473070\n",
            "\n",
            "Ilyoun\n",
            "Ina\n",
            "Iyuto\n",
            "Iha\n",
            "Yuto\n",
            "Ad\n",
            "Uusan\n",
            "\n",
            "\n",
            "Iteration: 57000, Loss: 15.422754\n",
            "\n",
            "Inyuun\n",
            "Hna\n",
            "Huuso\n",
            "Iha\n",
            "Uuto\n",
            "Ae\n",
            "Uura\n",
            "\n",
            "\n",
            "Iteration: 58000, Loss: 15.718355\n",
            "\n",
            "Ilyuun\n",
            "Higa\n",
            "Huuto\n",
            "Ie\n",
            "Uuto\n",
            "Ad\n",
            "Uus\n",
            "\n",
            "\n",
            "Iteration: 59000, Loss: 15.924742\n",
            "\n",
            "Ilutroai\n",
            "Ina\n",
            "Iyuto\n",
            "Iea\n",
            "Wsuraka\n",
            "Ebahmol\n",
            "Uusan\n",
            "\n",
            "\n",
            "Iteration: 60000, Loss: 15.991799\n",
            "\n",
            "Ikuto\n",
            "Ina\n",
            "Iyusubi\n",
            "Ida\n",
            "Uuto\n",
            "Ec\n",
            "Uusaki\n",
            "\n",
            "\n",
            "Iteration: 61000, Loss: 15.689520\n",
            "\n",
            "Ilyton\n",
            "Ina\n",
            "Iyusran\n",
            "Iea\n",
            "Vuura\n",
            "Ad\n",
            "Uusan\n",
            "\n",
            "\n",
            "Iteration: 62000, Loss: 15.705280\n",
            "\n",
            "Ilvossan\n",
            "Ina\n",
            "Izusramber\n",
            "Ie\n",
            "Yussalder\n",
            "Eb\n",
            "Uto\n",
            "\n",
            "\n",
            "Iteration: 63000, Loss: 15.749893\n",
            "\n",
            "Invousan\n",
            "Ina\n",
            "Izuus\n",
            "Ifa\n",
            "Yuus\n",
            "Ca\n",
            "Uusamaroethirs\n",
            "\n",
            "\n",
            "Iteration: 64000, Loss: 15.642386\n",
            "\n",
            "Imustofiane\n",
            "Ina\n",
            "Iyrun\n",
            "Ie\n",
            "Yuto\n",
            "Adachom\n",
            "Usramand\n",
            "\n",
            "\n",
            "Iteration: 65000, Loss: 15.974260\n",
            "\n",
            "Inyuun\n",
            "Ina\n",
            "Izuso\n",
            "Iha\n",
            "Yuura\n",
            "Ac\n",
            "Uusan\n",
            "\n",
            "\n",
            "Iteration: 66000, Loss: 15.970078\n",
            "\n",
            "Kmusrodedng\n",
            "Ioja\n",
            "Iyoshan\n",
            "Lea\n",
            "Yuto\n",
            "Ed\n",
            "Uurda\n",
            "\n",
            "\n",
            "Iteration: 67000, Loss: 15.876262\n",
            "\n",
            "Imuusran\n",
            "Incc\n",
            "Iyuurcharhatheri\n",
            "Iga\n",
            "Yuusan\n",
            "Ebaiorg\n",
            "Uure\n",
            "\n",
            "\n",
            "Iteration: 68000, Loss: 15.789286\n",
            "\n",
            "Ilwstohiemiaujilo\n",
            "Imba\n",
            "Iyuto\n",
            "Iga\n",
            "Wuto\n",
            "Ac\n",
            "Uun\n",
            "\n",
            "\n",
            "Iteration: 69000, Loss: 15.584547\n",
            "\n",
            "Ilysura\n",
            "Inca\n",
            "Iyuss\n",
            "Iea\n",
            "Yuto\n",
            "Da\n",
            "Uusaka\n",
            "\n",
            "\n",
            "Iteration: 70000, Loss: 15.782854\n",
            "\n",
            "Imuto\n",
            "Incc\n",
            "Iyuto\n",
            "Iha\n",
            "Yuura\n",
            "Ae\n",
            "Uura\n",
            "\n",
            "\n",
            "Iteration: 71000, Loss: 15.690473\n",
            "\n",
            "Ikuusoaa\n",
            "Hoka\n",
            "Iyuura\n",
            "Ie\n",
            "Uusobe\n",
            "Ae\n",
            "Uura\n",
            "\n",
            "\n",
            "Iteration: 72000, Loss: 15.826675\n",
            "\n",
            "Ilvotsan\n",
            "Inba\n",
            "Iyuso\n",
            "Ie\n",
            "Uuto\n",
            "Aca\n",
            "Uura\n",
            "\n",
            "\n",
            "Iteration: 73000, Loss: 15.791321\n",
            "\n",
            "Ilvoto\n",
            "Ina\n",
            "Iysuo\n",
            "Ie\n",
            "Uussan\n",
            "Ad\n",
            "Uuraa\n",
            "\n",
            "\n",
            "Iteration: 74000, Loss: 15.858147\n",
            "\n",
            "Ilvros\n",
            "Ima\n",
            "Izuso\n",
            "Iha\n",
            "Yusr\n",
            "Ae\n",
            "Uun\n",
            "\n",
            "\n",
            "Iteration: 75000, Loss: 15.854745\n",
            "\n",
            "Ilwrtran\n",
            "Ind\n",
            "Iyuto\n",
            "Iga\n",
            "Yuun\n",
            "Aca\n",
            "Uun\n",
            "\n",
            "\n",
            "Iteration: 76000, Loss: 15.722151\n",
            "\n",
            "Ilvotsimani\n",
            "Inca\n",
            "Iyuto\n",
            "Ida\n",
            "Uuti\n",
            "Acafstee\n",
            "Uusanannaterno\n",
            "\n",
            "\n",
            "Iteration: 77000, Loss: 15.810418\n",
            "\n",
            "Ilvouma\n",
            "Hiha\n",
            "Iyuto\n",
            "Iea\n",
            "Vrura\n",
            "Ad\n",
            "Uura\n",
            "\n",
            "\n",
            "Iteration: 78000, Loss: 15.801378\n",
            "\n",
            "Ilvosr\n",
            "Ina\n",
            "Iyuso\n",
            "Iga\n",
            "Uuso\n",
            "Ec\n",
            "Uurchan\n",
            "\n",
            "\n",
            "Iteration: 79000, Loss: 15.741811\n",
            "\n",
            "Imutod\n",
            "Hiha\n",
            "Huuto\n",
            "Iha\n",
            "Uuto\n",
            "Ad\n",
            "Utoae\n",
            "\n",
            "\n",
            "Iteration: 80000, Loss: 15.853585\n",
            "\n",
            "Ilvoto\n",
            "Hicc\n",
            "Huuso\n",
            "Ida\n",
            "Uuto\n",
            "Ac\n",
            "Uure\n",
            "\n",
            "\n",
            "Iteration: 81000, Loss: 15.736043\n",
            "\n",
            "Imutsu\n",
            "Hre\n",
            "Huuto\n",
            "Iha\n",
            "Vuto\n",
            "Ae\n",
            "Uusan\n",
            "\n",
            "\n",
            "Iteration: 82000, Loss: 15.847346\n",
            "\n",
            "Ilvotsan\n",
            "Ena\n",
            "Extor\n",
            "Ida\n",
            "Uusude\n",
            "Acabou\n",
            "Uura\n",
            "\n",
            "\n",
            "Iteration: 83000, Loss: 15.948224\n",
            "\n",
            "Imutsubi\n",
            "Incc\n",
            "Izustan\n",
            "Iha\n",
            "Yutpeo\n",
            "Cc\n",
            "Uusake\n",
            "\n",
            "\n",
            "Iteration: 84000, Loss: 15.962151\n",
            "\n",
            "Imusura\n",
            "Ina\n",
            "Iyusocher\n",
            "Iha\n",
            "Yussan\n",
            "Acaaotba\n",
            "Uun\n",
            "\n",
            "\n",
            "Iteration: 85000, Loss: 15.879194\n",
            "\n",
            "Ilvoslai\n",
            "Ine\n",
            "Iyuto\n",
            "Ijh\n",
            "Vuto\n",
            "Ed\n",
            "Uun\n",
            "\n",
            "\n",
            "Iteration: 86000, Loss: 15.685718\n",
            "\n",
            "Ilvtsuaka\n",
            "Ima\n",
            "Iyuuran\n",
            "Iga\n",
            "Yuto\n",
            "Ed\n",
            "Uura\n",
            "\n",
            "\n",
            "Iteration: 87000, Loss: 15.554100\n",
            "\n",
            "Imusti\n",
            "Hiha\n",
            "Huusranaliatango\n",
            "Ii\n",
            "Yuto\n",
            "Ad\n",
            "Uura\n",
            "\n",
            "\n",
            "Iteration: 88000, Loss: 15.592124\n",
            "\n",
            "Imussofkancesasro\n",
            "Eoh\n",
            "Hutsube\n",
            "Ii\n",
            "Vuto\n",
            "Ad\n",
            "Uusamand\n",
            "\n",
            "\n",
            "Iteration: 89000, Loss: 15.634862\n",
            "\n",
            "Imuto\n",
            "Hica\n",
            "Huuso\n",
            "Ifa\n",
            "Yuss\n",
            "Ad\n",
            "Uus\n",
            "\n",
            "\n",
            "Iteration: 90000, Loss: 15.961430\n",
            "\n",
            "Ilvprl\n",
            "Higa\n",
            "Iyusranane\n",
            "Iga\n",
            "Yustan\n",
            "Ad\n",
            "Uura\n",
            "\n",
            "\n",
            "Iteration: 91000, Loss: 15.780630\n",
            "\n",
            "Ilvoss\n",
            "Homa\n",
            "Iyusue\n",
            "Ie\n",
            "Wuto\n",
            "Ad\n",
            "Uura\n",
            "\n",
            "\n",
            "Iteration: 92000, Loss: 15.785370\n",
            "\n",
            "Imutoa\n",
            "Erea\n",
            "Gyrroaland\n",
            "Iha\n",
            "Yuss\n",
            "Acaburg\n",
            "Uura\n",
            "\n",
            "\n",
            "Iteration: 93000, Loss: 15.627814\n",
            "\n",
            "Invoujel\n",
            "Hii\n",
            "Huusue\n",
            "Iha\n",
            "Uusuhi\n",
            "Ad\n",
            "Uura\n",
            "\n",
            "\n",
            "Iteration: 94000, Loss: 15.612736\n",
            "\n",
            "Inusura\n",
            "Hid\n",
            "Hutsu\n",
            "Iha\n",
            "Yuto\n",
            "Ac\n",
            "Uusamaro\n",
            "\n",
            "\n",
            "Iteration: 95000, Loss: 15.615988\n",
            "\n",
            "Ilvotsel\n",
            "Ina\n",
            "Iyuss\n",
            "Ie\n",
            "Wrsoan\n",
            "Ec\n",
            "Uura\n",
            "\n",
            "\n",
            "Iteration: 96000, Loss: 15.859215\n",
            "\n",
            "Ikutsu\n",
            "Ina\n",
            "Izuss\n",
            "Ie\n",
            "Vutsimang\n",
            "Adako\n",
            "Uusake\n",
            "\n",
            "\n",
            "Iteration: 97000, Loss: 15.523927\n",
            "\n",
            "Inwoumae\n",
            "Hii\n",
            "Iyust\n",
            "Ifa\n",
            "Yuuscharl\n",
            "Ad\n",
            "Uusamder\n",
            "\n",
            "\n",
            "Iteration: 98000, Loss: 15.671297\n",
            "\n",
            "Ikustofl\n",
            "Ima\n",
            "Iyusranami\n",
            "Icha\n",
            "Yussan\n",
            "Acaftre\n",
            "Uuo\n",
            "\n",
            "\n",
            "Iteration: 99000, Loss: 15.897495\n",
            "\n",
            "Ilvoto\n",
            "Hija\n",
            "Iyuto\n",
            "Ifa\n",
            "Uustapari\n",
            "Ad\n",
            "Uura\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AkPZymRtFi92"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}